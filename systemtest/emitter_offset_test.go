package systemtest

import (
	"fmt"
	"log"
	"sync/atomic"
	"testing"
	"time"

	"github.com/Shopify/sarama"
	"github.com/lovoo/goka"
	"github.com/lovoo/goka/codec"
	"github.com/stretchr/testify/require"
)

// TestEmitterOffset is a simple brute force test that ensure
// that the offsets, generated by kafka producers are monotonically increasing.
// That is required to get rid of the former functionality "storeNewestOffset", that would ignore
// older offsets.
func TestEmitterOffset(t *testing.T) {
	var topic goka.Stream = goka.Stream(fmt.Sprintf("%s-%d", "goka-systemtest-emitter-offset", time.Now().Unix()))

	brokers := initSystemTest(t)

	tmc := goka.NewTopicManagerConfig()
	tmc.Table.Replication = 1
	cfg := goka.DefaultConfig()
	tm, err := goka.TopicManagerBuilderWithConfig(cfg, tmc)(brokers)
	require.NoError(t, err)
	tm.EnsureStreamExists(string(topic), 1)

	var lastOffset int64

	emitter, err := goka.NewEmitter(brokers, topic, new(codec.Int64))
	if err != nil {
		t.Fatalf("error creating emitter: %v", err)
	}

	for i := 0; i < 100000; i++ {
		prom, err := emitter.Emit(fmt.Sprintf("%d", i), int64(i))
		if err != nil {
			log.Fatalf("error emitting: %v", err)
		}
		prom.ThenWithMessage(func(msg *sarama.ProducerMessage, err error) {
			if err != nil {
				log.Fatalf("error emitting message: %v", err)
			}
			oldOffset := atomic.SwapInt64(&lastOffset, msg.Offset)
			if msg.Offset < oldOffset {
				log.Fatalf("offsets appeared in wrong order new=%d, old=%d", msg.Offset, oldOffset)
			}
		})
	}

	defer func() {
		if err := emitter.Finish(); err != nil {
			log.Fatalf("error closing emitter: %v", err)
		}
	}()
}
